{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyPDF2 in /opt/anaconda3/lib/python3.11/site-packages (3.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: nltk in /opt/anaconda3/lib/python3.11/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /opt/anaconda3/lib/python3.11/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/lib/python3.11/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/anaconda3/lib/python3.11/site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.11/site-packages (from nltk) (4.65.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting sklearn\n",
      "  Using cached sklearn-0.0.post12.tar.gz (2.6 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[15 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m The 'sklearn' PyPI package is deprecated, use 'scikit-learn'\n",
      "  \u001b[31m   \u001b[0m rather than 'sklearn' for pip commands.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m Here is how to fix this error in the main use cases:\n",
      "  \u001b[31m   \u001b[0m - use 'pip install scikit-learn' rather than 'pip install sklearn'\n",
      "  \u001b[31m   \u001b[0m - replace 'sklearn' by 'scikit-learn' in your pip requirements files\n",
      "  \u001b[31m   \u001b[0m   (requirements.txt, setup.py, setup.cfg, Pipfile, etc ...)\n",
      "  \u001b[31m   \u001b[0m - if the 'sklearn' package is used by one of your dependencies,\n",
      "  \u001b[31m   \u001b[0m   it would be great if you take some time to track which package uses\n",
      "  \u001b[31m   \u001b[0m   'sklearn' instead of 'scikit-learn' and report it to their issue tracker\n",
      "  \u001b[31m   \u001b[0m - as a last resort, set the environment variable\n",
      "  \u001b[31m   \u001b[0m   SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True to avoid this error\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m More information is available at\n",
      "  \u001b[31m   \u001b[0m https://github.com/scikit-learn/sklearn-pypi-package\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[?25h\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
      "\u001b[1;36mhint\u001b[0m: See above for details.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: openai in /opt/anaconda3/lib/python3.11/site-packages (1.30.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/anaconda3/lib/python3.11/site-packages (from openai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/lib/python3.11/site-packages (from openai) (1.8.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/lib/python3.11/site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/anaconda3/lib/python3.11/site-packages (from openai) (1.10.12)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/lib/python3.11/site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /opt/anaconda3/lib/python3.11/site-packages (from openai) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /opt/anaconda3/lib/python3.11/site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/anaconda3/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.4)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/lib/python3.11/site-packages (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/anaconda3/lib/python3.11/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /opt/anaconda3/lib/python3.11/site-packages (from scikit-learn) (1.11.4)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/anaconda3/lib/python3.11/site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from scikit-learn) (2.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: streamlit in /opt/anaconda3/lib/python3.11/site-packages (1.34.0)\n",
      "Requirement already satisfied: python-dotenv in /opt/anaconda3/lib/python3.11/site-packages (0.21.0)\n",
      "Requirement already satisfied: PyPDF2 in /opt/anaconda3/lib/python3.11/site-packages (3.0.1)\n",
      "Requirement already satisfied: langchain-openai in /opt/anaconda3/lib/python3.11/site-packages (0.1.7)\n",
      "Requirement already satisfied: langchain-community in /opt/anaconda3/lib/python3.11/site-packages (0.2.0)\n",
      "Requirement already satisfied: altair<6,>=4.0 in /opt/anaconda3/lib/python3.11/site-packages (from streamlit) (5.0.1)\n",
      "Requirement already satisfied: blinker<2,>=1.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from streamlit) (1.6.2)\n",
      "Requirement already satisfied: cachetools<6,>=4.0 in /opt/anaconda3/lib/python3.11/site-packages (from streamlit) (4.2.2)\n",
      "Requirement already satisfied: click<9,>=7.0 in /opt/anaconda3/lib/python3.11/site-packages (from streamlit) (8.1.7)\n",
      "Requirement already satisfied: numpy<2,>=1.19.3 in /opt/anaconda3/lib/python3.11/site-packages (from streamlit) (1.26.4)\n",
      "Requirement already satisfied: packaging<25,>=16.8 in /opt/anaconda3/lib/python3.11/site-packages (from streamlit) (23.2)\n",
      "Requirement already satisfied: pandas<3,>=1.3.0 in /opt/anaconda3/lib/python3.11/site-packages (from streamlit) (2.1.4)\n",
      "Requirement already satisfied: pillow<11,>=7.1.0 in /opt/anaconda3/lib/python3.11/site-packages (from streamlit) (10.2.0)\n",
      "Requirement already satisfied: protobuf<5,>=3.20 in /opt/anaconda3/lib/python3.11/site-packages (from streamlit) (3.20.3)\n",
      "Requirement already satisfied: pyarrow>=7.0 in /opt/anaconda3/lib/python3.11/site-packages (from streamlit) (14.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.27 in /opt/anaconda3/lib/python3.11/site-packages (from streamlit) (2.31.0)\n",
      "Requirement already satisfied: rich<14,>=10.14.0 in /opt/anaconda3/lib/python3.11/site-packages (from streamlit) (13.3.5)\n",
      "Requirement already satisfied: tenacity<9,>=8.1.0 in /opt/anaconda3/lib/python3.11/site-packages (from streamlit) (8.2.2)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in /opt/anaconda3/lib/python3.11/site-packages (from streamlit) (0.10.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /opt/anaconda3/lib/python3.11/site-packages (from streamlit) (4.9.0)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /opt/anaconda3/lib/python3.11/site-packages (from streamlit) (3.1.37)\n",
      "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /opt/anaconda3/lib/python3.11/site-packages (from streamlit) (0.8.0)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in /opt/anaconda3/lib/python3.11/site-packages (from streamlit) (6.3.3)\n",
      "Requirement already satisfied: langchain-core<0.3,>=0.1.46 in /opt/anaconda3/lib/python3.11/site-packages (from langchain-openai) (0.2.0)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.24.0 in /opt/anaconda3/lib/python3.11/site-packages (from langchain-openai) (1.30.1)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in /opt/anaconda3/lib/python3.11/site-packages (from langchain-openai) (0.7.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/anaconda3/lib/python3.11/site-packages (from langchain-community) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/anaconda3/lib/python3.11/site-packages (from langchain-community) (2.0.25)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/anaconda3/lib/python3.11/site-packages (from langchain-community) (3.9.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/anaconda3/lib/python3.11/site-packages (from langchain-community) (0.6.6)\n",
      "Requirement already satisfied: langchain<0.3.0,>=0.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from langchain-community) (0.2.0)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /opt/anaconda3/lib/python3.11/site-packages (from langchain-community) (0.1.60)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/anaconda3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.9.3)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.11/site-packages (from altair<6,>=4.0->streamlit) (3.1.3)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /opt/anaconda3/lib/python3.11/site-packages (from altair<6,>=4.0->streamlit) (4.19.2)\n",
      "Requirement already satisfied: toolz in /opt/anaconda3/lib/python3.11/site-packages (from altair<6,>=4.0->streamlit) (0.12.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/anaconda3/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.21.2)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/anaconda3/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/anaconda3/lib/python3.11/site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.7)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from langchain<0.3.0,>=0.2.0->langchain-community) (0.2.0)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /opt/anaconda3/lib/python3.11/site-packages (from langchain<0.3.0,>=0.2.0->langchain-community) (1.10.12)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/anaconda3/lib/python3.11/site-packages (from langchain-core<0.3,>=0.1.46->langchain-openai) (1.33)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/anaconda3/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.0->langchain-community) (3.10.3)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/anaconda3/lib/python3.11/site-packages (from openai<2.0.0,>=1.24.0->langchain-openai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/lib/python3.11/site-packages (from openai<2.0.0,>=1.24.0->langchain-openai) (1.8.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/lib/python3.11/site-packages (from openai<2.0.0,>=1.24.0->langchain-openai) (0.27.0)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/lib/python3.11/site-packages (from openai<2.0.0,>=1.24.0->langchain-openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /opt/anaconda3/lib/python3.11/site-packages (from openai<2.0.0,>=1.24.0->langchain-openai) (4.65.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.11/site-packages (from pandas<3,>=1.3.0->streamlit) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.11/site-packages (from pandas<3,>=1.3.0->streamlit) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/anaconda3/lib/python3.11/site-packages (from pandas<3,>=1.3.0->streamlit) (2023.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.27->streamlit) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.27->streamlit) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.27->streamlit) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.27->streamlit) (2024.2.2)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from rich<14,>=10.14.0->streamlit) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.11/site-packages (from rich<14,>=10.14.0->streamlit) (2.15.1)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/anaconda3/lib/python3.11/site-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.0.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/anaconda3/lib/python3.11/site-packages (from tiktoken<1,>=0.7->langchain-openai) (2023.10.3)\n",
      "Requirement already satisfied: smmap<5,>=3.0.1 in /opt/anaconda3/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.0)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.24.0->langchain-openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.24.0->langchain-openai) (0.14.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.11/site-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.3)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/anaconda3/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.1.46->langchain-openai) (2.1)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/anaconda3/lib/python3.11/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/anaconda3/lib/python3.11/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/anaconda3/lib/python3.11/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.10.6)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.11/site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas<3,>=1.3.0->streamlit) (1.16.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/anaconda3/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: transformers in /opt/anaconda3/lib/python3.11/site-packages (4.41.0)\n",
      "Requirement already satisfied: torch in /opt/anaconda3/lib/python3.11/site-packages (2.2.2)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.11/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /opt/anaconda3/lib/python3.11/site-packages (from transformers) (0.23.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.11/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.11/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/anaconda3/lib/python3.11/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/anaconda3/lib/python3.11/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/lib/python3.11/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/anaconda3/lib/python3.11/site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in /opt/anaconda3/lib/python3.11/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.11/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.11/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.11/site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.11/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.11/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.11/site-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.11/site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/anaconda3/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "%pip install PyPDF2\n",
    "%pip install nltk\n",
    "%pip install sklearn\n",
    "%pip install openai\n",
    "%pip install scikit-learn\n",
    "%pip install streamlit python-dotenv PyPDF2 langchain-openai langchain-community \n",
    "%pip install transformers torch\n",
    "%pip install nltk rouge-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain_text_splitters.character import CharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.embeddings import HuggingFaceInstructEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.llms import HuggingFaceHub\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Path to the PDF file\n",
    "pdf_path = \"path/Sample.pdf\"\n",
    "# Load API key from environment variables\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"OPENAI_API_KEY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the purpose of this document?\n",
      "Response: {'question': 'What is the purpose of this document?', 'chat_history': [HumanMessage(content='What is the purpose of this document?'), AIMessage(content='The purpose of the document is to provide information about the training corpus used for language models, specifically focusing on financial documents like company filings, press releases, and Bloomberg news. It also discusses the evaluation process for language models and the importance of domain-specific evaluations.')], 'answer': 'The purpose of the document is to provide information about the training corpus used for language models, specifically focusing on financial documents like company filings, press releases, and Bloomberg news. It also discusses the evaluation process for language models and the importance of domain-specific evaluations.'}\n",
      "\n",
      "Question: What is the purpose of this document?\n",
      "Response: {'question': 'What is the purpose of this document?', 'chat_history': [HumanMessage(content='What is the purpose of this document?'), AIMessage(content='The purpose of the document is to provide information about the training corpus used for language models, specifically focusing on financial documents like company filings, press releases, and Bloomberg news. It also discusses the evaluation process for language models and the importance of domain-specific evaluations.'), HumanMessage(content='What is the purpose of this document?'), AIMessage(content='The document provides detailed information about the training corpus used for language models. It mentions datasets like The Pile, C4, and Wikipedia, highlighting their token counts, cleaning processes, and the diversity of data sources included. It also discusses the tokenization process, the choice of tokenizer, and the importance of training data quality for language model performance. Additionally, it touches on the benefits of in-domain pretraining and the challenges of constructing high-quality training corpora.')], 'answer': 'The document provides detailed information about the training corpus used for language models. It mentions datasets like The Pile, C4, and Wikipedia, highlighting their token counts, cleaning processes, and the diversity of data sources included. It also discusses the tokenization process, the choice of tokenizer, and the importance of training data quality for language model performance. Additionally, it touches on the benefits of in-domain pretraining and the challenges of constructing high-quality training corpora.'}\n",
      "\n",
      "Evaluation Metrics:\n",
      "Average Cosine Similarity: 0.84\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def get_pdf_text(pdf_path):\n",
    "    \"\"\"Extract text from a PDF file.\"\"\"\n",
    "    text = \"\"\n",
    "    pdf_reader = PdfReader(pdf_path)\n",
    "    for page in pdf_reader.pages:\n",
    "        text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "def get_text_chunks(text):\n",
    "    \"\"\"Split text into manageable chunks.\"\"\"\n",
    "    text_splitter = CharacterTextSplitter(\n",
    "        separator=\"\\n\",\n",
    "        chunk_size=3000,  # Increase chunk size\n",
    "        chunk_overlap=1000,  # Increase overlap\n",
    "        length_function=len\n",
    "    )\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    return chunks\n",
    "\n",
    "def get_vectorstore(text_chunks):\n",
    "    \"\"\"Create a vector store from text chunks using embeddings.\"\"\"\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    # embeddings = HuggingFaceInstructEmbeddings(model_name=\"hkunlp/instructor-xl\")\n",
    "    vectorstore = FAISS.from_texts(texts=text_chunks, embedding=embeddings)\n",
    "    return vectorstore\n",
    "\n",
    "def get_conversation_chain(vectorstore):\n",
    "    \"\"\"Set up the conversational chain with memory.\"\"\"\n",
    "    llm = ChatOpenAI(temperature=0)\n",
    "    # llm = HuggingFaceHub(repo_id=\"google/flan-t5-xxl\", model_kwargs={\"temperature\":0.5, \"max_length\":512})\n",
    "\n",
    "    memory = ConversationBufferMemory(\n",
    "        memory_key='chat_history', return_messages=True)\n",
    "    conversation_chain = ConversationalRetrievalChain.from_llm(\n",
    "        llm=llm,\n",
    "        retriever=vectorstore.as_retriever(),\n",
    "        memory=memory\n",
    "    )\n",
    "    return conversation_chain\n",
    "\n",
    "def get_embedding(text, tokenizer, model):\n",
    "    \"\"\"Get BERT embedding for a given text.\"\"\"\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=512)\n",
    "    outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "\n",
    "def evaluate_model(conversation_chain, questions, true_answers):\n",
    "    \"\"\"Evaluate the model's performance using semantic similarity.\"\"\"\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    predicted_answers = []\n",
    "    for question in questions:\n",
    "        response = conversation_chain({'question': question})\n",
    "        print(f\"Question: {question}\")\n",
    "        print(f\"Response: {response}\\n\")\n",
    "        if 'answer' in response:\n",
    "            predicted_answers.append(response['answer'])\n",
    "        elif 'text' in response:\n",
    "            predicted_answers.append(response['text'])\n",
    "        else:\n",
    "            predicted_answers.append(\"\")\n",
    "\n",
    "    # Calculate cosine similarity between true and predicted answers\n",
    "    similarities = []\n",
    "    for true, pred in zip(true_answers, predicted_answers):\n",
    "        true_embedding = get_embedding(true, tokenizer, model)\n",
    "        pred_embedding = get_embedding(pred, tokenizer, model)\n",
    "        similarity = cosine_similarity(true_embedding, pred_embedding)[0][0]\n",
    "        similarities.append(similarity)\n",
    "\n",
    "    # Print average similarity\n",
    "    avg_similarity = sum(similarities) / len(similarities)\n",
    "    print(\"Evaluation Metrics:\")\n",
    "    print(f\"Average Cosine Similarity: {avg_similarity:.2f}\")\n",
    "\n",
    "# Extract text from the PDF\n",
    "raw_text = get_pdf_text(pdf_path)\n",
    "\n",
    "# Split text into chunks\n",
    "text_chunks = get_text_chunks(raw_text)\n",
    "\n",
    "# Create vector store from text chunks\n",
    "vectorstore = get_vectorstore(text_chunks)\n",
    "\n",
    "# Create conversation chain\n",
    "conversation_chain = get_conversation_chain(vectorstore)\n",
    "\n",
    "# Ask a question\n",
    "question = \"What is the purpose of this document?\"\n",
    "response = conversation_chain({'question': question})\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Response: {response}\\n\")\n",
    "\n",
    "# Example evaluation (replace with actual questions and answers)\n",
    "sample_questions = [\"What is the purpose of this document?\"]\n",
    "true_answers = [\"The purpose of this document is to present BloombergGPT, a large language model for finance, and validate its performance on various benchmarks.\"]  # Replace with actual expected answer\n",
    "evaluate_model(conversation_chain, sample_questions, true_answers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/reeyadav/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the purpose of this document?\n",
      "Response: {'question': 'What is the purpose of this document?', 'chat_history': [HumanMessage(content='What is the purpose of this document?'), AIMessage(content='The purpose of the document is to provide information about the training corpus used for language models, specifically focusing on financial documents like company filings, press releases, and Bloomberg news. It also discusses the evaluation process for language models and the importance of domain-specific evaluations.')], 'answer': 'The purpose of the document is to provide information about the training corpus used for language models, specifically focusing on financial documents like company filings, press releases, and Bloomberg news. It also discusses the evaluation process for language models and the importance of domain-specific evaluations.'}\n",
      "\n",
      "Question: What is the purpose of this document?\n",
      "Response: {'question': 'What is the purpose of this document?', 'chat_history': [HumanMessage(content='What is the purpose of this document?'), AIMessage(content='The purpose of the document is to provide information about the training corpus used for language models, specifically focusing on financial documents like company filings, press releases, and Bloomberg news. It also discusses the evaluation process for language models and the importance of domain-specific evaluations.'), HumanMessage(content='What is the purpose of this document?'), AIMessage(content='The document provides information about the training corpus used for language models, specifically for BloombergGPT, which is called \"FinPile.\" FinPile is a comprehensive dataset consisting of various English financial documents such as news, filings, press releases, web-scraped financial documents, and social media sourced from the Bloomberg archives. Additionally, the document mentions that FinPile is augmented with public data commonly used to train large language models.\\n\\nRegarding the evaluation process for language models, the document discusses the challenges and evolving nature of evaluating language models. It mentions different paradigms for evaluating language models, including automatic evaluation in various scenarios and extrinsic and task-specific evaluations integrated into user workflows. The document also highlights the importance of domain-specific evaluations and the need for aligning evaluation metrics with actual use cases. Additionally, it mentions the use of public financial NLP benchmarks and internal Bloomberg tasks for evaluating the model\\'s performance.')], 'answer': 'The document provides information about the training corpus used for language models, specifically for BloombergGPT, which is called \"FinPile.\" FinPile is a comprehensive dataset consisting of various English financial documents such as news, filings, press releases, web-scraped financial documents, and social media sourced from the Bloomberg archives. Additionally, the document mentions that FinPile is augmented with public data commonly used to train large language models.\\n\\nRegarding the evaluation process for language models, the document discusses the challenges and evolving nature of evaluating language models. It mentions different paradigms for evaluating language models, including automatic evaluation in various scenarios and extrinsic and task-specific evaluations integrated into user workflows. The document also highlights the importance of domain-specific evaluations and the need for aligning evaluation metrics with actual use cases. Additionally, it mentions the use of public financial NLP benchmarks and internal Bloomberg tasks for evaluating the model\\'s performance.'}\n",
      "\n",
      "Evaluation Metrics:\n",
      "Average Cosine Similarity: 0.88\n",
      "Average METEOR Score: 0.24\n",
      "Average ROUGE Scores: {'rouge1': 0.1744186046511628, 'rouge2': 0.023529411764705885, 'rougeL': 0.13953488372093023}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/anaconda3/lib/python3.11/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_pdf_text(pdf_path):\n",
    "    \"\"\"Extract text from a PDF file.\"\"\"\n",
    "    text = \"\"\n",
    "    pdf_reader = PdfReader(pdf_path)\n",
    "    for page in pdf_reader.pages:\n",
    "        text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "def get_text_chunks(text):\n",
    "    \"\"\"Split text into manageable chunks.\"\"\"\n",
    "    text_splitter = CharacterTextSplitter(\n",
    "        separator=\"\\n\",\n",
    "        chunk_size=3000,  # Increase chunk size\n",
    "        chunk_overlap=1000,  # Increase overlap\n",
    "        length_function=len\n",
    "    )\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    return chunks\n",
    "\n",
    "def get_vectorstore(text_chunks):\n",
    "    \"\"\"Create a vector store from text chunks using embeddings.\"\"\"\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    # embeddings = HuggingFaceInstructEmbeddings(model_name=\"hkunlp/instructor-xl\")\n",
    "    vectorstore = FAISS.from_texts(texts=text_chunks, embedding=embeddings)\n",
    "    return vectorstore\n",
    "\n",
    "def get_conversation_chain(vectorstore):\n",
    "    \"\"\"Set up the conversational chain with memory.\"\"\"\n",
    "    llm = ChatOpenAI(temperature=0)\n",
    "    # llm = HuggingFaceHub(repo_id=\"google/flan-t5-xxl\", model_kwargs={\"temperature\":0.5, \"max_length\":512})\n",
    "\n",
    "    memory = ConversationBufferMemory(\n",
    "        memory_key='chat_history', return_messages=True)\n",
    "    conversation_chain = ConversationalRetrievalChain.from_llm(\n",
    "        llm=llm,\n",
    "        retriever=vectorstore.as_retriever(),\n",
    "        memory=memory\n",
    "    )\n",
    "    return conversation_chain\n",
    "\n",
    "def get_embedding(text, tokenizer, model):\n",
    "    \"\"\"Get BERT embedding for a given text.\"\"\"\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=512)\n",
    "    outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "\n",
    "def evaluate_model(conversation_chain, questions, true_answers):\n",
    "    \"\"\"Evaluate the model's performance using semantic similarity, BLEU, ROUGE, and METEOR.\"\"\"\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    predicted_answers = []\n",
    "    for question in questions:\n",
    "        response = conversation_chain({'question': question})\n",
    "        print(f\"Question: {question}\")\n",
    "        print(f\"Response: {response}\\n\")\n",
    "        if 'answer' in response:\n",
    "            predicted_answers.append(response['answer'])\n",
    "        elif 'text' in response:\n",
    "            predicted_answers.append(response['text'])\n",
    "        else:\n",
    "            predicted_answers.append(\"\")\n",
    "\n",
    "    # Calculate cosine similarity between true and predicted answers\n",
    "    similarities = []\n",
    "    for true, pred in zip(true_answers, predicted_answers):\n",
    "        true_embedding = get_embedding(true, tokenizer, model)\n",
    "        pred_embedding = get_embedding(pred, tokenizer, model)\n",
    "        similarity = cosine_similarity(true_embedding, pred_embedding)[0][0]\n",
    "        similarities.append(similarity)\n",
    "\n",
    "    # Calculate average cosine similarity\n",
    "    avg_similarity = sum(similarities) / len(similarities)\n",
    "\n",
    "    # Initialize metrics\n",
    "    bleu_scores = []\n",
    "    meteor_scores = []\n",
    "    rouge_scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n",
    "    rouge_scorer_obj = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "    for true, pred in zip(true_answers, predicted_answers):\n",
    "        # Tokenize true and predicted answers\n",
    "        true_tokens = word_tokenize(true)\n",
    "        pred_tokens = word_tokenize(pred)\n",
    "        \n",
    "        # BLEU score\n",
    "        bleu_scores.append(sentence_bleu([true_tokens], pred_tokens))\n",
    "        \n",
    "        # METEOR score\n",
    "        meteor_scores.append(meteor_score([true_tokens], pred_tokens))\n",
    "        \n",
    "        # ROUGE score\n",
    "        rouge = rouge_scorer_obj.score(true, pred)\n",
    "        for key in rouge_scores:\n",
    "            rouge_scores[key].append(rouge[key].fmeasure)\n",
    "\n",
    "    # Calculate average scores\n",
    "    avg_bleu = sum(bleu_scores) / len(bleu_scores)\n",
    "    avg_meteor = sum(meteor_scores) / len(meteor_scores)\n",
    "    avg_rouge = {key: sum(values) / len(values) for key, values in rouge_scores.items()}\n",
    "\n",
    "    # Print evaluation metrics\n",
    "    print(\"Evaluation Metrics:\")\n",
    "    print(f\"Average Cosine Similarity: {avg_similarity:.2f}\")\n",
    "    print(f\"Average METEOR Score: {avg_meteor:.2f}\")\n",
    "    print(f\"Average ROUGE Scores: {avg_rouge}\")\n",
    "\n",
    "\n",
    "# Extract text from the PDF\n",
    "raw_text = get_pdf_text(pdf_path)\n",
    "\n",
    "# Split text into chunks\n",
    "text_chunks = get_text_chunks(raw_text)\n",
    "\n",
    "# Create vector store from text chunks\n",
    "vectorstore = get_vectorstore(text_chunks)\n",
    "\n",
    "# Create conversation chain\n",
    "conversation_chain = get_conversation_chain(vectorstore)\n",
    "\n",
    "# Ask a question\n",
    "question = \"What is the purpose of this document?\"\n",
    "response = conversation_chain({'question': question})\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Response: {response}\\n\")\n",
    "\n",
    "# Example evaluation (replace with actual questions and answers)\n",
    "sample_questions = [\"What is the purpose of this document?\"]\n",
    "true_answers = [\"The purpose of this document is to present BloombergGPT, a large language model for finance, and validate its performance on various benchmarks.\"]  # Replace with actual expected answer\n",
    "evaluate_model(conversation_chain, sample_questions, true_answers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
